{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f3779c",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "You will use the motivating example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below.  Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n",
    "\n",
    "You will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.  \n",
    "\n",
    "Please run the following code cell to create your `x_train` and `y_train` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c194d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b135ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Output set declaration\n",
    "x_train=np.array([[2104,5,1,45],[1416,3,2,40],[852,2,1,35]])\n",
    "y_train=np.array([460,232,178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e9201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing weights and bias\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "b_init = 785.1811367994083"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4297e8b4",
   "metadata": {},
   "source": [
    "### Prediction/Predicted Output, $$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b   $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fcb63ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction funtion that returns predicted output y = f w,b(x)\n",
    "def predict(w,b,x):\n",
    "    p = np.dot(w,x)+b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdab9ed",
   "metadata": {},
   "source": [
    "### Cost Function,  $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 $$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b   $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23c2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function.\n",
    "def compute_cost(x,y,w,b):\n",
    "    m=x.shape[0]\n",
    "    \n",
    "    cost = 0.0\n",
    "     \n",
    "    for i in range(m):\n",
    "        f_wb_i = predict(w,b,x[i])\n",
    "        cost +=(f_wb_i-y[i])**2\n",
    "        \n",
    "    cost = cost/(2*m)\n",
    "    \n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4824bc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5578904045996674e-12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost=compute_cost(x_train,y_train,w_init,b_init)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773ff888",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1372c960",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655ff51",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"resources\\images\\gradient_descent.png\"    style=\" width:280px; padding: 10px;  \" /> \n",
    "\n",
    "### Compute Gradient\n",
    "- outer loop over all m examples. \n",
    "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated\n",
    "    - in a second loop over all n features:\n",
    "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "610744db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating error - Predicted_Output - Given_Output\n",
    "\"\"\"\n",
    "Here w,b,x and y 1D numpy arrays\n",
    "\"\"\"\n",
    "def calculate_error(x,y,w,b):\n",
    "    error=predict(w,b,x) - y\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e1191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def calculate_gradient(x,y,w,b):\n",
    "    m,n=x.shape    #(number of examples, number of features)\n",
    "    dj_dw=np.zeros((n,)) # initially all the dj/dw are 0\n",
    "    dj_db=0.\n",
    "    \n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    for i in range(m):\n",
    "        error=predict(w,b,x[i]) - y[i]  # predict = f w,b(x[i]) = wx[i] + b\n",
    "    \n",
    "        for j in range(n):\n",
    "            dj_dw[i]=dj_dw[j]+error*x[i,j]\n",
    "        \n",
    "        dj_db=dj_db+error\n",
    "        \n",
    "    dj_dw=dj_dw/m\n",
    "    dj_db=dj_db/m\n",
    "    \n",
    "    return dj_db,dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a4be5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  gradient_descent(x,y,w_in,b_in,alpha,num_iters):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "    \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    j_history = []\n",
    "    \n",
    "    w=copy.deepcopy(w_in)\n",
    "    b=b_in\n",
    "    \n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = calculate_gradient(x, y, w, b)   ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "        \n",
    "    \n",
    "    return w,b,j_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f8decd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "b,w found by gradient descent: -0.00,[ 0.21946227 -0.38661683  0.02068947  0.        ] \n",
      "prediction: 459.83, target value: 460\n",
      "prediction: 309.64, target value: 232\n",
      "prediction: 186.23, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "print(initial_w)\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, j_hist = gradient_descent(x_train,y_train,initial_w,initial_b,alpha,iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = x_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(x_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
