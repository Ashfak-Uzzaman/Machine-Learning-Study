{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Lab: Linear Regression using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an open-source, commercially usable machine learning toolkit called [scikit-learn](https://scikit-learn.org/stable/index.html). This toolkit contains implementations of many of the algorithms that you will work with in this course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "In this lab you will:\n",
    "- Utilize  scikit-learn to implement linear regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "You will utilize functions from scikit-learn as well as matplotlib and NumPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "'''\n",
    "from lab_utils_multi import  load_house_data\n",
    "from lab_utils_common import dlc\n",
    "'''\n",
    "np.set_printoptions(precision=2)\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "Scikit-learn has a gradient descent regression model [sklearn.linear_model.SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#examples-using-sklearn-linear-model-sgdregressor).  Like your previous implementation of gradient descent, this model performs best with normalized inputs. [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) will perform z-score normalization as in a previous lab. Here it is referred to as 'standard score'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1.24e+03, 3.00e+00, 1.00e+00, 6.40e+01],\n",
    "                    [1.95e+03, 3.00e+00, 2.00e+00, 1.70e+01],\n",
    "                    [1.72e+03, 3.00e+00, 2.00e+00, 4.20e+01],\n",
    "                    [1.96e+03, 3.00e+00, 2.00e+00, 1.50e+01],\n",
    "                    [1.31e+03, 2.00e+00, 1.00e+00, 1.40e+01],\n",
    "                    [8.64e+02, 2.00e+00, 1.00e+00, 6.60e+01],\n",
    "                    [1.84e+03, 3.00e+00, 1.00e+00, 1.70e+01],\n",
    "                    [1.03e+03, 3.00e+00, 1.00e+00, 4.30e+01],\n",
    "                    [3.19e+03, 4.00e+00, 2.00e+00, 8.70e+01],\n",
    "                    [7.88e+02, 2.00e+00, 1.00e+00, 8.00e+01],\n",
    "                    [1.20e+03, 2.00e+00, 2.00e+00, 1.70e+01],\n",
    "                    [1.56e+03, 2.00e+00, 1.00e+00, 1.80e+01],\n",
    "                    [1.43e+03, 3.00e+00, 1.00e+00, 2.00e+01],\n",
    "                    [1.22e+03, 2.00e+00, 1.00e+00, 1.50e+01],\n",
    "                    [1.09e+03, 2.00e+00, 1.00e+00, 6.40e+01],\n",
    "                    [8.48e+02, 1.00e+00, 1.00e+00, 1.70e+01],\n",
    "                    [1.68e+03, 3.00e+00, 2.00e+00, 2.30e+01],\n",
    "                    [1.77e+03, 3.00e+00, 2.00e+00, 1.80e+01],\n",
    "                    [1.04e+03, 3.00e+00, 1.00e+00, 4.40e+01],\n",
    "                    [1.65e+03, 2.00e+00, 1.00e+00, 2.10e+01],\n",
    "                    [1.09e+03, 2.00e+00, 1.00e+00, 3.50e+01],\n",
    "                    [1.32e+03, 3.00e+00, 1.00e+00, 1.40e+01],\n",
    "                    [1.59e+03, 0.00e+00, 1.00e+00, 2.00e+01],\n",
    "                    [9.72e+02, 2.00e+00, 1.00e+00, 7.30e+01],\n",
    "                    [1.10e+03, 3.00e+00, 1.00e+00, 3.70e+01],\n",
    "                    [1.00e+03, 2.00e+00, 1.00e+00, 5.10e+01],\n",
    "                    [9.04e+02, 3.00e+00, 1.00e+00, 5.50e+01],\n",
    "                    [1.69e+03, 3.00e+00, 1.00e+00, 1.30e+01],\n",
    "                    [1.07e+03, 2.00e+00, 1.00e+00, 1.00e+02],\n",
    "                    [1.42e+03, 3.00e+00, 2.00e+00, 1.90e+01],\n",
    "                    [1.16e+03, 3.00e+00, 1.00e+00, 5.20e+01],\n",
    "                    [1.94e+03, 3.00e+00, 2.00e+00, 1.20e+01],\n",
    "                    [1.22e+03, 2.00e+00, 2.00e+00, 7.40e+01],\n",
    "                    [2.48e+03, 4.00e+00, 2.00e+00, 1.60e+01],\n",
    "                    [1.20e+03, 2.00e+00, 1.00e+00, 1.80e+01],\n",
    "                    [1.84e+03, 3.00e+00, 2.00e+00, 2.00e+01],\n",
    "                    [1.85e+03, 3.00e+00, 2.00e+00, 5.70e+01],\n",
    "                    [1.66e+03, 3.00e+00, 2.00e+00, 1.90e+01],\n",
    "                    [1.10e+03, 2.00e+00, 2.00e+00, 9.70e+01],\n",
    "                    [1.78e+03, 3.00e+00, 2.00e+00, 2.80e+01],\n",
    "                    [2.03e+03, 4.00e+00, 2.00e+00, 4.50e+01],\n",
    "                    [1.78e+03, 4.00e+00, 2.00e+00, 1.07e+02],\n",
    "                    [1.07e+03, 2.00e+00, 1.00e+00, 1.00e+02],\n",
    "                    [1.55e+03, 3.00e+00, 1.00e+00, 1.60e+01],\n",
    "                    [1.95e+03, 3.00e+00, 2.00e+00, 1.60e+01],\n",
    "                    [1.22e+03, 2.00e+00, 2.00e+00, 1.20e+01],\n",
    "                    [1.62e+03, 3.00e+00, 1.00e+00, 1.60e+01],\n",
    "                    [8.16e+02, 2.00e+00, 1.00e+00, 5.80e+01],\n",
    "                    [1.35e+03, 3.00e+00, 1.00e+00, 2.10e+01],\n",
    "                    [1.57e+03, 3.00e+00, 1.00e+00, 1.40e+01],\n",
    "                    [1.49e+03, 3.00e+00, 1.00e+00, 5.70e+01],\n",
    "                    [1.51e+03, 2.00e+00, 1.00e+00, 1.60e+01],\n",
    "                    [1.10e+03, 3.00e+00, 1.00e+00, 2.70e+01],\n",
    "                    [1.76e+03, 3.00e+00, 2.00e+00, 2.40e+01],\n",
    "                    [1.21e+03, 2.00e+00, 1.00e+00, 1.40e+01],\n",
    "                    [1.47e+03, 3.00e+00, 2.00e+00, 2.40e+01],\n",
    "                    [1.77e+03, 3.00e+00, 2.00e+00, 8.40e+01],\n",
    "                    [1.65e+03, 3.00e+00, 1.00e+00, 1.90e+01],\n",
    "                    [1.03e+03, 3.00e+00, 1.00e+00, 6.00e+01],\n",
    "                    [1.12e+03, 2.00e+00, 2.00e+00, 1.60e+01],\n",
    "                    [1.15e+03, 3.00e+00, 1.00e+00, 6.20e+01],\n",
    "                    [8.16e+02, 2.00e+00, 1.00e+00, 3.90e+01],\n",
    "                    [1.04e+03, 3.00e+00, 1.00e+00, 2.50e+01],\n",
    "                    [1.39e+03, 3.00e+00, 1.00e+00, 6.40e+01],\n",
    "                    [1.60e+03, 3.00e+00, 2.00e+00, 2.90e+01],\n",
    "                    [1.22e+03, 3.00e+00, 1.00e+00, 6.30e+01],\n",
    "                    [1.07e+03, 2.00e+00, 1.00e+00, 1.00e+02],\n",
    "                    [2.60e+03, 4.00e+00, 2.00e+00, 2.20e+01],\n",
    "                    [1.43e+03, 3.00e+00, 1.00e+00, 5.90e+01],\n",
    "                    [2.09e+03, 3.00e+00, 2.00e+00, 2.60e+01],\n",
    "                    [1.79e+03, 4.00e+00, 2.00e+00, 4.90e+01],\n",
    "                    [1.48e+03, 3.00e+00, 2.00e+00, 1.60e+01],\n",
    "                    [1.04e+03, 3.00e+00, 1.00e+00, 2.50e+01],\n",
    "                    [1.43e+03, 3.00e+00, 1.00e+00, 2.20e+01],\n",
    "                    [1.16e+03, 3.00e+00, 1.00e+00, 5.30e+01],\n",
    "                    [1.55e+03, 3.00e+00, 2.00e+00, 1.20e+01],\n",
    "                    [1.98e+03, 3.00e+00, 2.00e+00, 2.20e+01],\n",
    "                    [1.06e+03, 3.00e+00, 1.00e+00, 5.30e+01],\n",
    "                    [1.18e+03, 2.00e+00, 1.00e+00, 9.90e+01],\n",
    "                    [1.36e+03, 2.00e+00, 1.00e+00, 1.70e+01],\n",
    "                    [9.60e+02, 3.00e+00, 1.00e+00, 5.10e+01],\n",
    "                    [1.46e+03, 3.00e+00, 2.00e+00, 1.60e+01],\n",
    "                    [1.45e+03, 3.00e+00, 2.00e+00, 2.50e+01],\n",
    "                    [1.21e+03, 2.00e+00, 1.00e+00, 1.50e+01],\n",
    "                    [1.55e+03, 3.00e+00, 2.00e+00, 1.60e+01],\n",
    "                    [8.82e+02, 3.00e+00, 1.00e+00, 4.90e+01],\n",
    "                    [2.03e+03, 4.00e+00, 2.00e+00, 4.50e+01],\n",
    "                    [1.04e+03, 3.00e+00, 1.00e+00, 6.20e+01],\n",
    "                    [1.62e+03, 3.00e+00, 1.00e+00, 1.60e+01],\n",
    "                    [8.03e+02, 2.00e+00, 1.00e+00, 8.00e+01],\n",
    "                    [1.43e+03, 3.00e+00, 2.00e+00, 2.10e+01],\n",
    "                    [1.66e+03, 3.00e+00, 1.00e+00, 6.10e+01],\n",
    "                    [1.54e+03, 3.00e+00, 1.00e+00, 1.60e+01],\n",
    "                    [9.48e+02, 3.00e+00, 1.00e+00, 5.30e+01],\n",
    "                    [1.22e+03, 2.00e+00, 2.00e+00, 1.20e+01],\n",
    "                    [1.43e+03, 2.00e+00, 1.00e+00, 4.30e+01],\n",
    "                    [1.66e+03, 3.00e+00, 2.00e+00, 1.90e+01],\n",
    "                    [1.21e+03, 3.00e+00, 1.00e+00, 2.00e+01],\n",
    "                    [1.05e+03, 2.00e+00, 1.00e+00, 6.50e+01]])\n",
    "\n",
    "y_train = np.array([300., 509.8, 394., 540., 415., 230., 560., 294., 718.2, 200.,\n",
    "                    302., 468., 374.2, 388., 282., 311.8, 401., 449.8, 301., 502.,\n",
    "                    340., 400.28, 572., 264., 304., 298., 219.8, 490.7, 216.96, 368.2,\n",
    "                    280., 526.87, 237., 562.43, 369.8, 460., 374., 390., 158., 426.,\n",
    "                    390., 277.77, 216.96, 425.8, 504., 329., 464., 220., 358., 478.,\n",
    "                    334., 426.98, 290., 463., 390.8, 354., 350., 460., 237., 288.3,\n",
    "                    282., 249., 304., 332., 351.8, 310., 216.96, 666.34, 330., 480.,\n",
    "                    330.3, 348., 304., 384., 316., 430.4, 450., 284., 275., 414.,\n",
    "                    258., 378., 350., 412., 373., 225., 390., 267.4, 464., 174.,\n",
    "                    340., 430., 440., 216., 329., 388., 390., 356., 257.8])\n",
    "\n",
    "X_features = ['size(sqft)','bedrooms','floors','age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale/normalize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X_train)\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and fit the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdr = SGDRegressor(max_iter=1000)\n",
    "sgdr.fit(X_norm, y_train)\n",
    "print(sgdr)\n",
    "print(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View parameters\n",
    "Note, the parameters are associated with the *normalized* input data. The fit parameters are very close to those found in the previous lab with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_norm = sgdr.intercept_\n",
    "w_norm = sgdr.coef_\n",
    "print(f\"model parameters:                   w: {w_norm}, b:{b_norm}\")\n",
    "print( \"model parameters from previous lab: w: [110.56 -21.27 -32.71 -37.97], b: 363.16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions\n",
    "Predict the targets of the training data. Use both the `predict` routine and compute using $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction using sgdr.predict()\n",
    "y_pred_sgd = sgdr.predict(X_norm)\n",
    "# make a prediction using w,b. \n",
    "y_pred = np.dot(X_norm, w_norm) + b_norm  \n",
    "print(f\"prediction using np.dot() and sgdr.predict match: {(y_pred == y_pred_sgd).all()}\")\n",
    "\n",
    "print(f\"Prediction on training set:\\n{y_pred[:4]}\" )\n",
    "print(f\"Target values \\n{y_train[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results\n",
    "Let's plot the predictions versus the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions and targets vs original features    \n",
    "fig,ax=plt.subplots(1,4,figsize=(12,3),sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i],y_train, label = 'target')\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "    ax[i].scatter(X_train[:,i],y_pred,color=dlc[\"dlorange\"], label = 'predict')\n",
    "ax[0].set_ylabel(\"Price\"); ax[0].legend();\n",
    "fig.suptitle(\"target versus prediction using z-score normalized model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "In this lab you:\n",
    "- utilized an open-source machine learning toolkit, scikit-learn\n",
    "- implemented linear regression using gradient descent and feature normalization from that toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
