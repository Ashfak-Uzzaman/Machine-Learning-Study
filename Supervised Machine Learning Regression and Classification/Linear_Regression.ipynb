{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0001e44b",
   "metadata": {},
   "source": [
    "# Linerar Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea41192",
   "metadata": {},
   "source": [
    "## -  Problem Statement\n",
    "\n",
    "Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet.\n",
    "- You would like to expand your business to cities that may give your restaurant higher profits.\n",
    "- The chain already has restaurants in various cities and you have data for profits and populations from the cities.\n",
    "- You also have data on cities that are candidates for a new restaurant. \n",
    "    - For these cities, you have the city population.\n",
    "    \n",
    "Can you use the data to help you identify which cities may potentially give your business higher profits?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df169b32",
   "metadata": {},
   "source": [
    "## 1 - Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b32c99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import copy\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db525cfa",
   "metadata": {},
   "source": [
    "\n",
    "## 2 - Dataset\n",
    "\n",
    "You will start by loading the dataset for this task. \n",
    "-  Loads the data into variables `x_train` and `y_train`\n",
    "  - `x_train` is the population of a city\n",
    "  - `y_train` is the profit of a restaurant in that city. A negative value for profit indicates a loss.   \n",
    "  - Both `X_train` and `y_train` are numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5469bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "x_train, y_train = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f7c80",
   "metadata": {},
   "source": [
    "#### Check the dimensions of your variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1447ec2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of x_train is: (97,)\n",
      "The shape of y_train is:  (97,)\n",
      "Number of training examples (m): 97\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of x_train is:', x_train.shape)\n",
    "print ('The shape of y_train is: ', y_train.shape)\n",
    "print ('Number of training examples (m):', len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce560a28",
   "metadata": {},
   "source": [
    "### `compute_cost` Function:\n",
    "\n",
    "* Iterate over the training examples, and for each example, compute:\n",
    "    * The prediction of the model for that example \n",
    "    $$\n",
    "    f_{wb}(x^{(i)}) =  wx^{(i)} + b \n",
    "    $$\n",
    "   \n",
    "    * The cost for that example  $$cost^{(i)} =  (f_{wb} - y^{(i)})^2$$\n",
    "    \n",
    "\n",
    "* Return the total cost over all examples\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} cost^{(i)}$$\n",
    "  * Here, $m$ is the number of training examples and $\\sum$ is the summation operator\n",
    "\n",
    "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3884c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Shape (m,) Input to the model (Population of cities) \n",
    "        y (ndarray): Shape (m,) Label (Actual profits for the cities)\n",
    "        w, b (scalar): Parameters of the model\n",
    "    \n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = x.shape[0] \n",
    "\n",
    "    total_cost = 0\n",
    "\n",
    "    cost=0.0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb_i=np.dot(w,x[i])+b\n",
    "        cost+=(f_wb_i-y[i])**2\n",
    "    \n",
    "    total_cost=cost/(2*m)\n",
    "    \n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23560398",
   "metadata": {},
   "source": [
    "### `compute_gradient` Function:\n",
    "\n",
    "* Iterate over the training examples, and for each example, compute:\n",
    "    * The prediction of the model for that example \n",
    "    $$\n",
    "    f_{wb}(x^{(i)}) =  wx^{(i)} + b \n",
    "    $$\n",
    "   \n",
    "    * The gradient for the parameters $w, b$ from that example \n",
    "        $$\n",
    "        \\frac{\\partial J(w,b)}{\\partial b}^{(i)}  =  (f_{w,b}(x^{(i)}) - y^{(i)}) \n",
    "        $$\n",
    "        $$\n",
    "        \\frac{\\partial J(w,b)}{\\partial w}^{(i)}  =  (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)} \n",
    "        $$\n",
    "    \n",
    "\n",
    "* Return the total gradient update from all the examples\n",
    "    $$\n",
    "    \\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{\\partial J(w,b)}{\\partial b}^{(i)}\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "    \\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{\\partial J(w,b)}{\\partial w}^{(i)} \n",
    "    $$\n",
    "  * Here, $m$ is the number of training examples and $\\sum$ is the summation operator\n",
    "\n",
    "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0eb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: compute_gradient\n",
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) Input to the model (Population of cities) \n",
    "      y (ndarray): Shape (m,) Label (Actual profits for the cities)\n",
    "      w, b (scalar): Parameters of the model  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]\n",
    "  \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(m):\n",
    "        f_wb = w*x[i]+b\n",
    "        dj_dw+=(f_wb-y[i])*x[i]\n",
    "        dj_db+=(f_wb-y[i])\n",
    "        \n",
    "    dj_dw=dj_dw/m;\n",
    "    dj_db=dj_db/m\n",
    "        \n",
    "\n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6b6a3",
   "metadata": {},
   "source": [
    "### Learning parameters using batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0130806e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d621d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x :    (ndarray): Shape (m,)\n",
    "      y :    (ndarray): Shape (m,)\n",
    "      w_in, b_in : (scalar) Initial values of parameters of the model\n",
    "      cost_function: function to compute cost\n",
    "      gradient_function: function to compute the gradient\n",
    "      alpha : (float) Learning rate\n",
    "      num_iters : (int) number of iterations to run gradient descent\n",
    "    Returns\n",
    "      w : (ndarray): Shape (1,) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(x)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration â€” primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b )  \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(x, y, w, b)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            w_history.append(w)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ecec20",
   "metadata": {},
   "source": [
    "Now let's run the gradient descent algorithm above to learn the parameters for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6f5b076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     6.74   \n",
      "Iteration  250: Cost     5.06   \n",
      "Iteration  500: Cost     4.71   \n",
      "Iteration  750: Cost     4.57   \n",
      "Iteration 1000: Cost     4.52   \n",
      "Iteration 1250: Cost     4.49   \n",
      "Iteration 1500: Cost     4.48   \n",
      "Iteration 1750: Cost     4.48   \n",
      "Iteration 2000: Cost     4.48   \n",
      "Iteration 2250: Cost     4.48   \n",
      "w,b found by gradient descent: 1.1886434853952736 -3.8520806813360893\n"
     ]
    }
   ],
   "source": [
    "# initialize fitting parameters. Recall that the shape of w is (n,)\n",
    "initial_w = 0.\n",
    "initial_b = 0.\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 2500\n",
    "alpha = 0.01\n",
    "\n",
    "w,b,_,_ = gradient_descent(x_train ,y_train, initial_w, initial_b, \n",
    "                     compute_cost, compute_gradient, alpha, iterations)\n",
    "print(\"w,b found by gradient descent:\", w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989c76e9",
   "metadata": {},
   "source": [
    "we can get the prediction for a single example $f(x^{(i)})= wx^{(i)}+b$. \n",
    "\n",
    "To calculate the predictions on the entire dataset, we can loop through all the training examples and calculate the prediction for each example. This is shown in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14bc86e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.592    9.1302  13.662   11.854    6.8233  11.886    4.3483  12.\n",
      "  6.5987   3.8166   3.2522  15.505    3.1551   7.2258   0.71618  3.5129\n",
      "  5.3048   0.56077  3.6518   5.3893   3.1386  21.767    4.263    5.1875\n",
      "  3.0825  22.638   13.501    7.0467  14.692   24.147   -1.22     5.9966\n",
      " 12.134    1.8495   6.5426   4.5623   4.1164   3.3928  10.117    5.4974\n",
      "  0.55657  3.9115   5.3854   2.4406   6.7318   1.0463   5.1337   1.844\n",
      "  8.0043   1.0179   6.7504   1.8396   4.2885   4.9981   1.4233  -1.4211\n",
      "  2.4756   4.6042   3.9624   5.4141   5.1694  -0.74279 17.929   12.054\n",
      " 17.054    4.8852   5.7442   7.7754   1.0173  20.992    6.6799   4.0259\n",
      "  1.2784   3.3411  -2.6807   0.29678  3.8845   5.7014   6.7526   2.0576\n",
      "  0.47953  0.20421  0.67861  7.5435   5.3436   4.2415   6.7981   0.92695\n",
      "  0.152    2.8214   1.8451   4.2959   7.2029   1.9869   0.14454  9.0551\n",
      "  0.61705]\n",
      "[ 3.41064988  2.71838391  6.27349771  4.47222738  3.11313241  6.11219879\n",
      "  5.03469347  6.344222    3.85769869  2.15603668  2.93590567 12.98386565\n",
      "  2.96360106  6.1425092   2.85270063  2.54210808  3.71411056  2.24577926\n",
      "  3.79042147  4.55257968  3.50455271 20.24172277  2.67369092  3.66739687\n",
      "  2.76260145 18.66677015 11.39583795  9.17188599 11.80948588 22.53937062\n",
      "  2.39115036  3.9803667   7.140732    3.15116901  5.90798984  5.57790355\n",
      "  5.77105811  2.81181129 11.4053471   3.69984684  2.57479578  4.32875811\n",
      " 10.06455725  3.01079021  5.448698    4.57908642  2.17457952  3.04371563\n",
      " 10.0550481   2.73490606  5.11052893  2.45688235  4.97228969  5.1852946\n",
      "  3.67536078  3.70638438  3.60570627  2.85151198  7.2144279   7.38487937\n",
      "  6.63817353  2.30426052 21.44106404 13.8682164  18.68341116  4.72778572\n",
      "  6.00783589  8.31487404  2.6847453  20.32611646  8.19600969  4.86602496\n",
      "  3.28714982  4.73693828  2.12311126  3.931038    5.1086271   2.13452223\n",
      "  8.36004249  2.21915365  2.95789558  2.31507718  3.70258072  7.75942093\n",
      "  3.89300141  6.27183361  7.05990424  3.28215752  2.70970682  2.16174217\n",
      "  2.93233974  5.22511416  3.12608863  2.45414847  6.0058152  12.06861016\n",
      "  2.61045508]\n"
     ]
    }
   ],
   "source": [
    "m = x_train.shape[0]\n",
    "predicted = np.zeros(m)\n",
    "\n",
    "for i in range(m):\n",
    "    predicted[i] = w * x_train[i] + b\n",
    "    \n",
    "print(y_train)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28219557",
   "metadata": {},
   "source": [
    "#### Now For population = 35,000 and for population = 70,000, we will predict profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16670274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For population = 35,000, we predict a profit of $3081.72\n",
      "For population = 70,000, we predict a profit of $44684.24\n"
     ]
    }
   ],
   "source": [
    "predict1 = 3.5 * w + b\n",
    "print('For population = 35,000, we predict a profit of $%.2f' % (predict1*10000))\n",
    "\n",
    "predict2 = 7.0 * w + b\n",
    "print('For population = 70,000, we predict a profit of $%.2f' % (predict2*10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c7115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
